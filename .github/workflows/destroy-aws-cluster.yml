name: Destroy AWS Cluster (GitHub Actions)

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Type "destroy" to confirm'
        required: true
        default: ''

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
  PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

jobs:
  destroy:
    name: Destroy Cluster
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirm == 'destroy' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.AWS_ROLE_ARN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: pulumi-aws/package-lock.json

      - name: Install Pulumi CLI
        uses: pulumi/setup-pulumi@v2

      - name: Install dependencies
        working-directory: pulumi-aws
        run: npm ci

      - name: Select Pulumi stack
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        run: |
          set +e
          pulumi stack select dev
          if [ $? -ne 0 ]; then
            echo "Stack 'dev' not found. It may have already been destroyed."
            echo "Nothing to destroy. Exiting successfully."
            exit 0
          fi
          echo "Stack 'dev' selected successfully"

      - name: Cancel any in-progress operations
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        run: |
          set +e
          echo "Checking for in-progress operations..."
          # Intentar cancelar cualquier operación en progreso
          pulumi cancel || echo "No operations to cancel or already completed"
          # Esperar un poco para que se complete la cancelación
          sleep 5
          echo "Ready to proceed with destroy"

      - name: Remove all Kubernetes resources from state
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        run: |
          set +e
          echo "Removing all Kubernetes resources from state..."
          # Método 1: Intentar obtener recursos usando pulumi stack export
          echo "Method 1: Trying to get resources from stack export..."
          pulumi stack export 2>/dev/null > /tmp/stack.json
          if [ -f /tmp/stack.json ] && [ -s /tmp/stack.json ]; then
            echo "Extracting Kubernetes resource URNs from stack export..."
            jq -r '.deployment.resources[]? | select(.urn | contains("kubernetes:")) | .urn' /tmp/stack.json 2>/dev/null | while read URN; do
              if [ -n "$URN" ]; then
                echo "Removing Kubernetes resource: $URN"
                pulumi state delete "$URN" --yes 2>/dev/null && echo "✓ Deleted: $URN" || echo "⚠ Failed or already removed: $URN"
              fi
            done
          else
            echo "Could not export stack, using fallback method..."
          fi
          # Método 2: Fallback - eliminar recursos conocidos problemáticos
          echo "Method 2: Removing known problematic Kubernetes resources..."
          for URN in \
            "urn:pulumi:dev::todo-aws::custom:components:ClusterAutoscaler\$kubernetes:apps/v1:Deployment::cluster-autoscaler" \
            "urn:pulumi:dev::todo-aws::custom:components:ClusterAutoscaler\$kubernetes:core/v1:ServiceAccount::cluster-autoscaler" \
            "urn:pulumi:dev::todo-aws::custom:components:ClusterAutoscaler\$kubernetes:rbac.authorization.k8s.io/v1:ClusterRole::cluster-autoscaler" \
            "urn:pulumi:dev::todo-aws::custom:components:ClusterAutoscaler\$kubernetes:rbac.authorization.k8s.io/v1:ClusterRoleBinding::cluster-autoscaler" \
            "urn:pulumi:dev::todo-aws::custom:components:Kubernetes\$kubernetes:core/v1:Secret::mysql-secret" \
            "urn:pulumi:dev::todo-aws::custom:components:Kubernetes\$kubernetes:core/v1:ConfigMap::backend-config" \
            "urn:pulumi:dev::todo-aws::custom:components:Kubernetes\$kubernetes:core/v1:Namespace::todo-namespace" \
            "urn:pulumi:dev::todo-aws::custom:components:LoadBalancer\$kubernetes:core/v1:ServiceAccount::aws-load-balancer-controller"; do
            echo "Attempting to delete: $URN"
            pulumi state delete "$URN" --yes 2>/dev/null && echo "✓ Deleted: $URN" || echo "⚠ Not found: $URN"
          done
          echo "Kubernetes resources removal completed"

      - name: Destroy infrastructure
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
          PULUMI_K8S_DELETE_UNREACHABLE: "true"
        run: |
          set +e
          pulumi destroy --yes --skip-preview
          EXIT_CODE=$?
          if [ $EXIT_CODE -eq 255 ]; then
            # Error 404 significa que el stack no existe o ya fue eliminado
            echo "Stack may have already been destroyed (404 error). Checking..."
            pulumi stack select dev 2>&1 | grep -q "404" && echo "Stack already destroyed. Exiting successfully." && exit 0
          fi
          exit $EXIT_CODE

      - name: Show destroyed resources
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        run: pulumi stack --show-urns || echo "Stack destroyed or empty"
