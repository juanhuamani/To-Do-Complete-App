name: AWS Complete Deploy (Pulumi + ECR + EKS)

on:
  workflow_dispatch:
  push:
    branches: [ main ]

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
  PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
  PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }}
  PULUMI_DB_PASSWORD: ${{ secrets.PULUMI_DB_PASSWORD }}
  PULUMI_APP_KEY: ${{ secrets.PULUMI_APP_KEY }}

jobs:
  deploy:
    name: Provision + Build/Push + Deploy
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.AWS_ROLE_ARN }}

      - name: Setup Node.js (Pulumi project)
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: pulumi-aws/package-lock.json

      - name: Install Pulumi CLI
        uses: pulumi/setup-pulumi@v2

      - name: Install Pulumi dependencies
        working-directory: pulumi-aws
        run: npm ci

      - name: Select or create Pulumi stack
        id: pulumi_stack
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          STACK="dev"
          pulumi stack select "$STACK" || pulumi stack init "$STACK"
          echo "stack=$STACK" >> $GITHUB_OUTPUT

      - name: Pulumi config defaults
        working-directory: pulumi-aws
        shell: bash
        run: |
          pulumi config set awsRegion ${AWS_REGION} --non-interactive || true
          pulumi config set useExistingRepos true --non-interactive || true
          pulumi config set minNodes 2 --non-interactive || true
          pulumi config set maxNodes 3 --non-interactive || true
          pulumi config set instanceType t3.small --non-interactive || true
          # dbPassword (secreto requerido)
          # RDS solo acepta caracteres ASCII imprimibles excepto '/', '@', '"', y espacios
          if [ -n "${PULUMI_DB_PASSWORD}" ]; then
            pulumi config set --secret dbPassword "${PULUMI_DB_PASSWORD}" --non-interactive || true
          else
            echo "No se encontró PULUMI_DB_PASSWORD; generando uno aleatorio temporal" >&2
            # Generar contraseña sin caracteres prohibidos: /, @, ", espacio
            # Usar solo letras, números y caracteres especiales permitidos
            RAND_DB=$(openssl rand -base64 24 2>/dev/null | tr -d '/@" ' | head -c 32 || head -c 32 /dev/urandom | tr -d '/@" ' | base64 | tr -d '/@" ')
            pulumi config set --secret dbPassword "${RAND_DB}" --non-interactive || true
          fi
          # appKey (Laravel APP_KEY requerido)
          if [ -n "${PULUMI_APP_KEY}" ]; then
            pulumi config set --secret appKey "${PULUMI_APP_KEY}" --non-interactive || true
          else
            echo "No se encontró PULUMI_APP_KEY; generando uno tipo base64:" >&2
            GEN_APP_KEY="base64:$(openssl rand -base64 32 2>/dev/null || head -c 32 /dev/urandom | base64)"
            pulumi config set --secret appKey "${GEN_APP_KEY}" --non-interactive || true
          fi

      - name: Pulumi state cleanup for old ECR repos
        working-directory: pulumi-aws
        shell: bash
        run: |
          set +e
          # Elimina del estado los recursos antiguos para que no intente borrarlos (protegidos)
          FRONTEND_URN="urn:pulumi:dev::todo-aws::aws:ecr/repository:Repository::todo-frontend-repo"
          BACKEND_URN="urn:pulumi:dev::todo-aws::aws:ecr/repository:Repository::todo-backend-repo"
          pulumi state unprotect "$FRONTEND_URN" || true
          pulumi state delete "$FRONTEND_URN" -y || true
          pulumi state unprotect "$BACKEND_URN" || true
          pulumi state delete "$BACKEND_URN" -y || true

      - name: Skip import (useExistingRepos=true)
        run: echo "Repos ECR existentes serán referenciados, no importados"

      - name: Pulumi up (provision infra)
        working-directory: pulumi-aws
        env:
          PULUMI_ACCESS_TOKEN: ${{ env.PULUMI_ACCESS_TOKEN }}
        run: pulumi up --yes --skip-preview

      - name: Read Pulumi outputs
        id: pul_outputs
        working-directory: pulumi-aws
        shell: bash
        run: |
          set -euo pipefail
          CLUSTER=$(pulumi stack output clusterName)
          BACKEND_REPO=$(pulumi stack output backendRepoUrl)
          FRONTEND_REPO=$(pulumi stack output frontendRepoUrl)
          DB_HOST=$(pulumi stack output dbHost)
          REGION=$(pulumi stack output region)
          echo "cluster=$CLUSTER" >> $GITHUB_OUTPUT
          echo "backend_repo=$BACKEND_REPO" >> $GITHUB_OUTPUT
          echo "frontend_repo=$FRONTEND_REPO" >> $GITHUB_OUTPUT
          echo "db_host=$DB_HOST" >> $GITHUB_OUTPUT
          echo "region=$REGION" >> $GITHUB_OUTPUT

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Ensure ECR repositories
        shell: bash
        run: |
          set -euo pipefail
          for REPO in backend frontend; do
            aws ecr describe-repositories --repository-names "$REPO" >/dev/null 2>&1 || \
            aws ecr create-repository --repository-name "$REPO" --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE >/dev/null
          done

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & Push backend:latest
        uses: docker/build-push-action@v6
        with:
          context: backend
          file: backend/Dockerfile
          push: true
          tags: ${{ steps.pul_outputs.outputs.backend_repo }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build & Push frontend:latest (placeholder API)
        uses: docker/build-push-action@v6
        with:
          context: frontend
          file: frontend/Dockerfile
          push: true
          tags: ${{ steps.pul_outputs.outputs.frontend_repo }}:latest
          build-args: |
            VITE_API_URL=http://PLACEHOLDER/api
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region "${{ steps.pul_outputs.outputs.region }}" --name "${{ steps.pul_outputs.outputs.cluster }}"

      - name: Configure aws-auth ConfigMap (add GitHub Actions role and root user)
        shell: bash
        run: |
          set -euo pipefail
          CURRENT_ROLE_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
          echo "Current role ARN: $CURRENT_ROLE_ARN"
          ROLE_NAME=$(echo $CURRENT_ROLE_ARN | awk -F'/' '{print $NF}')
          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          ROOT_USER_ARN="arn:aws:iam::${ACCOUNT_ID}:root"
          echo "Root user ARN: $ROOT_USER_ARN"
          kubectl get configmap aws-auth -n kube-system -o yaml > /tmp/aws-auth.yaml
          UPDATED=false
          if ! kubectl get configmap aws-auth -n kube-system -o yaml 2>/dev/null | grep -q "$CURRENT_ROLE_ARN"; then
            echo "Adding GitHub Actions role to aws-auth ConfigMap..."
            if ! grep -q "mapRoles:" /tmp/aws-auth.yaml; then
              echo "  mapRoles: |" >> /tmp/aws-auth.yaml
            fi
            if ! grep -q "$CURRENT_ROLE_ARN" /tmp/aws-auth.yaml; then
              sed -i "/mapRoles: |/a\    - rolearn: $CURRENT_ROLE_ARN\n      username: $ROLE_NAME\n      groups:\n        - system:masters" /tmp/aws-auth.yaml
              UPDATED=true
            fi
          fi
          if ! kubectl get configmap aws-auth -n kube-system -o yaml 2>/dev/null | grep -q "$ROOT_USER_ARN"; then
            echo "Adding root user to aws-auth ConfigMap..."
            if ! grep -q "mapUsers:" /tmp/aws-auth.yaml; then
              echo "  mapUsers: |" >> /tmp/aws-auth.yaml
            fi
            if ! grep -q "$ROOT_USER_ARN" /tmp/aws-auth.yaml; then
              sed -i "/mapUsers: |/a\    - userarn: $ROOT_USER_ARN\n      username: root\n      groups:\n        - system:masters" /tmp/aws-auth.yaml
              UPDATED=true
            fi
          fi
          if [ "$UPDATED" = true ]; then
            kubectl apply -f /tmp/aws-auth.yaml
            echo "aws-auth ConfigMap updated successfully"
          else
            echo "aws-auth ConfigMap already configured"
          fi

      - name: Create namespace todo (if not exists)
        run: kubectl create namespace todo || true

      - name: Create/Update ECR pull secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n todo delete secret ecr-registry-secret || true
          TOKEN=$(aws ecr get-authorization-token --region $AWS_REGION --query 'authorizationData[0].authorizationToken' --output text)
          SERVER="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          kubectl -n todo create secret docker-registry ecr-registry-secret \
            --docker-server=https://$SERVER \
            --docker-username=AWS \
            --docker-password="$TOKEN"

      - name: Apply base manifests (ConfigMaps/Services)
        run: |
          kubectl apply -f k8s-aws/namespace.yaml
          kubectl apply -f k8s-aws/backend-configmap.yaml
          kubectl apply -f k8s-aws/frontend-configmap.yaml
          kubectl apply -f k8s-aws/backend-service.yaml
          kubectl apply -f k8s-aws/frontend-service.yaml

      - name: Apply deployments and set images
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f k8s-aws/backend-deployment.yaml
          kubectl -n todo set image deployment/backend backend="${{ steps.pul_outputs.outputs.backend_repo }}:latest" --record=true
          kubectl -n todo rollout status deployment/backend --timeout=5m

          kubectl apply -f k8s-aws/frontend-deployment.yaml
          kubectl -n todo set image deployment/frontend frontend="${{ steps.pul_outputs.outputs.frontend_repo }}:latest" --record=true
          kubectl -n todo rollout status deployment/frontend --timeout=5m

      - name: Get Backend LoadBalancer URL
        id: backend_lb
        shell: bash
        run: |
          set -euo pipefail
          ATTEMPTS=30
          for i in $(seq 1 $ATTEMPTS); do
            HOST=$(kubectl get service backend -n todo -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            PORT=$(kubectl get service backend -n todo -o jsonpath='{.spec.ports[0].port}' || echo 8000)
            if [ -n "$HOST" ] && [ "$HOST" != "null" ]; then
              echo "url=http://$HOST:$PORT" >> $GITHUB_OUTPUT
              echo "Found backend LB: http://$HOST:$PORT"
              exit 0
            fi
            echo "Waiting for backend LB... ($i/$ATTEMPTS)"
            sleep 10
          done
          echo "url=" >> $GITHUB_OUTPUT
          echo "Backend LB not ready"

      - name: Rebuild & Push frontend with real API URL
        if: ${{ steps.backend_lb.outputs.url != '' }}
        uses: docker/build-push-action@v6
        with:
          context: frontend
          file: frontend/Dockerfile
          push: true
          tags: ${{ steps.pul_outputs.outputs.frontend_repo }}:latest
          build-args: |
            VITE_API_URL=${{ steps.backend_lb.outputs.url }}/api
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Restart frontend deployment (new image)
        if: ${{ steps.backend_lb.outputs.url != '' }}
        run: |
          kubectl -n todo rollout restart deployment/frontend
          kubectl -n todo rollout status deployment/frontend --timeout=2m || true

      - name: Run migrations & seed
        run: |
          kubectl exec deployment/backend -n todo -- php artisan migrate --force
          kubectl exec deployment/backend -n todo -- php artisan db:seed --class=TaskSeeder --force

      - name: Show services and HPAs
        run: |
          kubectl get pods -n todo
          kubectl get services -n todo
          kubectl get hpa -n todo || true


